Kubernetes
We will be using kubectl for issuing cmd
We have to create a deployment config which tells how to create and update instances of your application
Kubectl uses the Kubernetes API to interact with the cluster
kubectl get nodes and version
kubectl create deployment name linktorepo
kubectl get deployments
A Pod is a group of one or more containers, with shared storage and network resources...
and a specification for how to run the containers.
Every Kubernetes Node runs at least:
    Kubelet, a process responsible for communication between the Kubernetes Master and the Node; ...
        it manages the Pods and the containers running on a machine.
    A container runtime (like Docker) responsible for pulling the container image from a registry,...
         unpacking the container, and running the application.
kubectl get - list resources - kubectl get pods
kubectl describe - show detailed information about a resource - kubectl describe pods
kubectl logs - print the logs from a container in a pod - kubectl logs $POD_NAME
kubectl exec - execute a command on a container in a pod - kubectl exec $POD_NAME env
service is used to expose the node/container etc
Services match a set of Pods using labels and selectors, a grouping primitive that allows logical ...
operation on objects in Kubernetes.
label is like a nickname to the deployment
kubectl expose deployment/kubernetes-bootcamp --type="NodePort" --port 8080
kubectl delete service -l run=kubernetes-bootcamp
Scaling when kubectlg get deployments is used we get some fields:
    NAME lists the names of the Deployments in the cluster.
    READY shows the ratio of CURRENT/DESIRED replicas
    UP-TO-DATE displays the number of replicas that have been updated to achieve the desired state.
    AVAILABLE displays how many replicas of the application are available to your users.
    AGE displays the amount of time that the application has been running.
Here Desired is the no of replicas user defined whike creating a deployment and the current ...
is the one which is running
kubectl scale deployments/kubernetes-bootcamp --replicas=4
for scaling down use kubectl scale and change --replicas=2 eg: kubectl scale deployments/kubernetes-bootcamp --replicas=2
Upadting:
    kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v2
    checking whether it has been updated:
        kubectl rollout status deployments/kubernetes-bootcamp
        kubectl rollout undo deployments/kubernetes-bootcamp


Kubernetes is used to run multiple containers .. the same can be done using ebs but we wont be having any access..
over the container which is being created ie if we waant to remove client/server/nginx from ebs in a container we wont...
be able to do that in ebs 
Kube consist of a cluster which has a master which gives commands to node... the node is vm/a computer which has a...
container running.
Dev - MiniKube
AWS - EKS
Google - GKE
mini kube for local env
We have to install kubectl to use it we need a vm and then we have to install MiniKube to create a cluster with a single node -> minikube start is used to start ....
minikube status - info to check whether things are working fine
kubectl cluster-info - info to check whether things are working fine
For kube we need the containers to be bulit already, a config file for creating a container and a config file for networking
Create the config file
    We use kind: pod /service is in order to create an object for serving different purpose,
    the apiVersion basically like from in docker which has the predefined objects like pod and service
    apiVersion: v1 has pod whereas apiVersion: apps/v1 has StatefulSet 
    We cant deploy a single container directly insided k8s so we have to create a pod to run a container ...
    thats why its called as a smallest deployable units of computing
    Services :
        some of the services types:
            clusterIp
            NodePort - Expose a container to outside world used only in dev env
            ingress
            loadbalancer
    the selector and label is used to make sure that the service routes it to that particular pod 
    if another pod tries to access other pod it can use port to access the pod
    targetPort is mapped to the pod port
    and the nodePort is the exposed port(if not specified random port will be generated)
kubectl apply -f fileName both the pod and service is configured.
once the above cmd is executed we can view the pods using kubectl get pods
This node is enclosed in a vm so we have to find the ip of the kube in order to access the pod
minikube ip to find the ip
use that ip to access the container running.
kubectl delete service client-node-port - delete a service
kubectl delete pod client-pod - delete a pod
if a docker container is killed, k8's automatically restart the pod.
When we apply a config file it tells the master to perform operations on nodes.
To prevent a node from scheduling new pods use: kubectl cordon <node-name>
To tell is to resume scheduling use: kubectl uncordon <node-name>
minikube service client-node-port to access the app.
Two Deployments :
        Imperative - Step By Step Instruction.
        Declarative - Make this happen.
The master update the pod/service using either Imperative or Declarative ..
In Imperative the user has to mention which pod to be updated.
Whereas in Declarative the user has to make changes in the config file without changing the name and the kind
if either of kind or name is changed the master thinks that the pod/service doesnt exist and create a new one.     
The updates can be checkedd by using  kubectl describe pod POD_NAME
kubectl delete -f client-pod.yaml to delete the pod/service/deployment created by the config file
The pods on crashing might be deleted or on updating the pod might change the ip . in either case the k8's master need a mechanism..
to update the ip of the pod which is deleted-created/updated for that service is used.
The service uses selector along with label to find out a particular pod is deleted so change the ip of that pod.
we can update by changing config file.
when the image is changed we need to make sure that the pod runs the latest version.
There isnt a proper solution for that yet , Imperative cmd or mentioning the tag in config must be done.
But using Imperative cmd is more better than mentioning tag option.
Change the tag version and push it into docker hub.
then use a Imperative cmd kubectl set image kind_name/name_of_deployment name_of_container=image:tag
    kubectl set image deployment/client-deployment client=vikassuresh/multi-client:v5
In order to access the docker server running inside a vm   we can use : $ eval $(minikube docker-env)
and then docker ps to display all the container (once the terminal is closed we have to use the cmd to get access to the server running in vm)

MultiContainer:

We are gonna create config files for each deployment and for each service for this a total of 11 along with postgress which has pvc (persistent volume claim)...
should be created
clusterIp allows containers to access the pods .. whereas the nodePort allows the user to access from outside, in case of clusterIp it isnt possible.
The user can access via ingress which access the clusterIp.
kubectl apply -f folderName to run all the config files inside the folder.
we can either create a single file where u can create the deployments and the service together along with --- to seperate
or u can provide it in seperate file. the latter gives u more flexiblity to find.
When using postgres it creates a pod with data when the pod gets destroyed the data inside the old pod wont be carried ovver the new pod ...
that is created by the kubernetes. For that only pvc is used.
Increasing the replicas in config files without some configurtions might create problems cause both pod is accessing same pvc and both pod doesnt know other pod exist.
Volume, persistent Volume and pvc are three different things. Volume in ku8 is not same as in docker.
Volume - Can survive container restarts but not pod restart.
Persistent Volume - Can survive pod restart.
PVC is just a way of letting know that these are the volumes which exist.
The persistent volume has statucally provisioned volume, if the claim has the volume then its used, if not ...
then it creates the claim in the persistent volume and its used.
AccessModes tell us how the nodes interact with the pv.
And the resources tells us the space allocated.
In order to use pvc , in postgres config file we have to mention to use the volume.
the volumes tells the kube should create a volume of that pvc config.
the volumeMounts tells the kube that it should use this particular volume.
  pv to find all the pv created.
kubectl get pvc to find all the pvc.
Add env inside containers in the specific deployment file.
For creating a secure place to store a value we can use Secret. Like a db pwd.
kubectl create secret generic pgpwd --from-literal PGPASSWORD=12345asdf to create secret.
kubectl get secrets to view secret.
in config file in env the secret is given inside valueFrom with the name being secret name and the key being the key of the value.
multiple key value pair can be stored inside the same secret.
Cannot pass integer have to wrap it in quotes(only string is allowed).
kubectl logs -f postgres-deployment-85677d544-97x76 to fetch the logs


loadbalancer and ingress:
loadbalancer is the legacy way of passing traffic to the service, the disadv is when it has multiple service...
it exposes multiple ip address
Ingress is a smart router which sits in front of multiple service and tells what to do.
if using ingress-nginx check th ingress-nginx and not kubernetes-ingress
Well be creating an ingress config which creates a ingress controller.
controller is something which run always to make sure the things in config files comes into reality, in other words..
the controller routes to corresponding route.
Important Note : Giving minikube start creates minikube within docker because of this the ingress thing wont work..
To make it work we have to create minikube with --vm=true command by deleteing the old minkube created with docker.
minikube delete and run with minikube start --vm=true. and then minikube addons enable ingress
Create a ingress-service.yaml file with configuration and run it... then we can access tje app via minikube ip.


Deployment in google-cloud.
Create an acc
Create a new Project.
Enable a billing account.
Then In compute KubeEngine -> Enable a Kube Engine.
Create a cluster.
Workloads tab shows all the pod/deployments.
Ingress/Service tab shows services.
Configuration tab shows secret and env values.
Application - different plugin running inside cluster.
storage - pvc/pv
The process is like this :
    install google cloud -> config the sdk with out auth info -> login dockercli -> build test version -> run test...
    -> deploy img -> push to docker -> apply all the config for k8s and imperatively fetch latest img.
For authenticating we need to create a service-account.json
For that create a service account-> download it in a json file -> install travis-cli ->
encrypt and upload the json file into travis-acc -> in travis add code to unencrypt the json file
and load it into gcloud sdk

Creating a service-acc
    IAM and Admin -> service account